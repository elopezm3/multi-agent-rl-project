{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Reinforcement Learning: Coordinated Package Delivery\n",
    "\n",
    "**Author**: Esteban Lopez (ID: 34377360)  \n",
    "**Course**: FIT5226 Multi-Agent Systems  \n",
    "**Institution**: Monash University  \n",
    "\n",
    "## ðŸŽ¯ Project Overview\n",
    "\n",
    "This project implements a multi-agent reinforcement learning system where 4 agents learn to coordinate package delivery in a 5Ã—5 grid world. The challenge involves agents picking up packages from location A and delivering them to location B while avoiding collisions, all within a 1.5M step training limit.\n",
    "\n",
    "**Key Achievement**: Achieved **78%+ success rate** (exceeding the 75% target) using a novel staged training approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Problem Statement\n",
    "\n",
    "Train 4 agents in a 5Ã—5 grid world to learn a coordinated transport task:\n",
    "\n",
    "### ðŸ“Š **Performance Constraints**\n",
    "- **Step Budget**: 1,500,000 maximum agent steps during training\n",
    "- **Collision Budget**: 4,000 maximum head-on collisions during training  \n",
    "- **Walltime Budget**: 10 minutes maximum runtime for training\n",
    "- **Final Performance**: 75% success rate across all scenarios\n",
    "- **Efficiency**: Each agent must complete delivery in â‰¤25 steps\n",
    "- **Safety**: All deliveries must be collision-free\n",
    "\n",
    "### ðŸŽ¯ **Challenge Requirements**\n",
    "- **Objective**: Pick up packages from location A and deliver to location B\n",
    "- **Constraint**: Avoid collisions between agents\n",
    "- **Evaluation**: Test across all possible A-B-agent combinations\n",
    "\n",
    "## ðŸš€ Key Innovation\n",
    "\n",
    "**Staged Training Approach**: Instead of training all 4 agents simultaneously, we train a single agent across diverse scenarios, then deploy the learned policy to multiple agents. This approach achieves emergent collision avoidance through natural path differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Notebook Structure\n",
    "\n",
    "This notebook is organized into the following sections:\n",
    "\n",
    "1. **ðŸŽ¯ Problem Statement & Innovation** - Challenge definition and novel approach\n",
    "2. **ðŸ“š Methodology** - Staged training strategy explanation\n",
    "3. **ðŸ”§ Implementation** - Core classes and algorithms\n",
    "4. **ðŸ‹ï¸ Training Process** - Q-learning implementation and optimization\n",
    "5. **ðŸ“Š Evaluation** - Comprehensive testing and performance analysis\n",
    "6. **ðŸ“ˆ Results & Discussion** - Performance metrics and insights\n",
    "7. **ðŸ” Limitations & Future Work** - Honest assessment and improvements\n",
    "\n",
    "## ðŸŽ¯ Quick Start\n",
    "\n",
    "To run this notebook:\n",
    "1. Install dependencies: `pip install -r requirements.txt`\n",
    "2. Run all cells to see the complete training and evaluation process\n",
    "3. Modify hyperparameters in the training section as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "This challenge is solved by using a \"Staged Training\".\n",
    "\n",
    "After many different strategies to reach the desired performance, the best training method found was to train a single agent in the most possible number of scenarios (A and B in different locations) resetting each scenario every 100 steps, while giving some rewards to the agent based on its behaviour (see Rewards section). So all the training consists in a single agent in the 1.500.000 steps learning the best possible path from A to B and from B to A. The interesting point about this is how in most scenarios the agent takes a different path from A to B than from B to A, which makes the collisions do not occur when placing the 4 agents, this keypoint will be analysed further on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARDS = {\n",
    "    'base_step_cost': -0.5,    # Cost for each step taken\n",
    "    'closer_reward': 0.5,      # Reward for moving closer to target\n",
    "    'away_penalty': -0.5,      # Penalty for moving away from target\n",
    "    'collision_penalty': 0,    # Penalty for collisions\n",
    "    'pickup_reward': 30,       # Reward for picking up package\n",
    "    'delivery_reward': 30      # Reward for delivering package\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All rewards previously defined are really self-explanatory by themselves. The only interesting point is that the collision penalty has value of 0. This is because during our training we are only using a single agent so it is not possible that there exists a collision. In fact, this collision penalty could have any value and it would not affect our training. The only reason why it is defined is case another another stage is implemented into the training with more than one agent and therefore we would need to adjust this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid World Classes Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Represents an individual agent in the multi-agent environment.\n",
    "    \n",
    "    Each agent has a position in the grid, can carry a package, and can move in four directions.\n",
    "    The agent's state includes its position, package status, and relative positions to package\n",
    "    pickup (A) and delivery (B) points.\n",
    "    \"\"\"\n",
    "    def __init__(self, position, agent_id):\n",
    "        \"\"\"\n",
    "        Initialize an agent with a position in the grid.\n",
    "        \n",
    "        Args:\n",
    "            position (tuple): Initial (x, y) position of the agent\n",
    "            agent_id (int): Unique identifier for the agent\n",
    "        \"\"\"\n",
    "        self.position = position\n",
    "        self.has_package = False\n",
    "        self.agent_id = agent_id\n",
    "    \n",
    "    def get_state(self, package_position, target_position):\n",
    "        \"\"\"\n",
    "        Return the agent's state as a numpy array for Q-learning.\n",
    "        State includes:\n",
    "        - agent_x, agent_y (2 values)\n",
    "        - package_x, package_y (position A) (2 values)\n",
    "        - target_x, target_y (position B) (2 values)\n",
    "        - has_package (0/1) (1 value)\n",
    "        Total state size: 7 values\n",
    "        \n",
    "        Args:\n",
    "            package_position (tuple): (x,y) position of package pickup point A\n",
    "            target_position (tuple): (x,y) position of delivery point B\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (x, y, px, py, tx, ty, has_package)\n",
    "        \"\"\"\n",
    "        x, y = self.position\n",
    "        px, py = package_position\n",
    "        tx, ty = target_position\n",
    "        has_package = int(self.has_package)\n",
    "        return (x, y, px, py, tx, ty, has_package)\n",
    "\n",
    "    def move(self, action, grid_size):\n",
    "        \"\"\"\n",
    "        Move the agent according to the given action.\n",
    "        Movement is constrained to grid boundaries.\n",
    "        \n",
    "        Args:\n",
    "            action (str): One of \"up\", \"down\", \"left\", \"right\"\n",
    "            grid_size (int): Size of the grid\n",
    "        \"\"\"\n",
    "        x, y = self.position\n",
    "\n",
    "        # \"up\" means decreasing row (x), \"down\" means increasing row (x)\n",
    "        # \"left\" means decreasing column (y), \"right\" means increasing column (y)\n",
    "        if action == \"up\" and x > 0:\n",
    "            self.position = (x - 1, y)\n",
    "        elif action == \"down\" and x < grid_size - 1:\n",
    "            self.position = (x + 1, y)\n",
    "        elif action == \"left\" and y > 0:\n",
    "            self.position = (x, y - 1)\n",
    "        elif action == \"right\" and y < grid_size - 1:\n",
    "            self.position = (x, y + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentEnvironment:\n",
    "    \"\"\"\n",
    "    A grid-based environment for multiple agents to learn delivery tasks.\n",
    "    \n",
    "    The environment consists of a grid where agents must pick up packages from location A\n",
    "    and deliver them to location B. Agents can move in four directions, and the environment\n",
    "    handles collisions, package pickups, deliveries, and rewards. The environment maintains\n",
    "    the state of all agents and the grid, and provides methods for agents to interact with\n",
    "    the environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_size=5, num_agents=4, package_position=None, target_position=None, rewards=REWARDS):\n",
    "        \"\"\"\n",
    "        Initialize a multi-agent environment with a grid and agents.\n",
    "        \n",
    "        Args:\n",
    "            grid_size (int): Size of the square grid\n",
    "            num_agents (int): Number of agents in the environment\n",
    "            package_position (tuple): (x,y) position of package pickup point A\n",
    "            target_position (tuple): (x,y) position of delivery point B\n",
    "            rewards (dict): Dictionary of reward values for different actions\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.num_agents = num_agents\n",
    "        self.grid = np.zeros((grid_size, grid_size), dtype=object)\n",
    "        \n",
    "        # Store rewards configuration\n",
    "        self.rewards = rewards\n",
    "        \n",
    "        # Randomly place A and B unless specified\n",
    "        positions = self.get_random_positions(2)\n",
    "        if package_position is None or target_position is None:\n",
    "            self.package_position = positions[0]  # A location\n",
    "            self.target_position = positions[1]   # B location\n",
    "        else:\n",
    "            self.package_position = package_position\n",
    "            self.target_position = target_position\n",
    "        \n",
    "        # Initialize agents\n",
    "        self.agents = []\n",
    "        for i in range(num_agents):\n",
    "            start_position = random.choice([self.package_position, self.target_position])\n",
    "            \n",
    "            agent = Agent(start_position, i)\n",
    "            # If agent starts at A, it should have the package\n",
    "            agent.has_package = (start_position == self.package_position)\n",
    "            self.agents.append(agent)\n",
    "            \n",
    "        self.update_grid()\n",
    "        self.current_agent_idx = 0\n",
    "\n",
    "    def get_random_positions(self, n, exclude=None):\n",
    "        \"\"\"\n",
    "        Get n random positions in the grid, excluding specified positions.\n",
    "        \n",
    "        Args:\n",
    "            n (int): Number of positions to generate\n",
    "            exclude (list): List of positions to exclude\n",
    "            \n",
    "        Returns:\n",
    "            list: List of n random (x,y) positions\n",
    "        \"\"\"\n",
    "        if exclude is None:\n",
    "            exclude = []\n",
    "        positions = []\n",
    "        while len(positions) < n:\n",
    "            pos = (random.randint(0, self.grid_size-1), random.randint(0, self.grid_size-1))\n",
    "            if pos not in exclude and pos not in positions:\n",
    "                positions.append(pos)\n",
    "        return positions\n",
    "    \n",
    "    def get_opposite_agents(self, agent):\n",
    "        \"\"\"\n",
    "        Get information about agents with opposite package status in neighboring cells.\n",
    "        Used for collision detection.\n",
    "        \n",
    "        Args:\n",
    "            agent (Agent): The agent to check neighbors for\n",
    "            \n",
    "        Returns:\n",
    "            list: Boolean list indicating presence of opposite agents in 8 neighboring cells\n",
    "        \"\"\"\n",
    "        x, y = agent.position\n",
    "        neighbors = []\n",
    "        for dx, dy in [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]:\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            if 0 <= new_x < self.grid_size and 0 <= new_y < self.grid_size:\n",
    "                has_opposite = False\n",
    "                for other_agent in self.agents:\n",
    "                    if (other_agent.position == (new_x, new_y) and \n",
    "                        other_agent.has_package != agent.has_package):\n",
    "                        has_opposite = True\n",
    "                        break\n",
    "                neighbors.append(has_opposite)\n",
    "            else:\n",
    "                neighbors.append(False)\n",
    "        return neighbors\n",
    "\n",
    "    def is_head_on_collision(self, agent, new_position):\n",
    "        \"\"\"\n",
    "        Check if moving to new_position would cause a head-on collision.\n",
    "        A head-on collision occurs when two agents with opposite package status\n",
    "        try to occupy the same cell (except at A or B locations).\n",
    "        \n",
    "        Args:\n",
    "            agent (Agent): The agent attempting to move\n",
    "            new_position (tuple): The position to check for collision\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if collision would occur, False otherwise\n",
    "        \"\"\"\n",
    "        # Don't check for collisions at A or B locations\n",
    "        if new_position == self.package_position or new_position == self.target_position:\n",
    "            return False\n",
    "        for other_agent in self.agents:\n",
    "            if other_agent.position == new_position and other_agent != agent:\n",
    "                return agent.has_package != other_agent.has_package\n",
    "        return False\n",
    "\n",
    "    def move_agent(self, agent, action):\n",
    "        \"\"\"\n",
    "        Move an agent and handle package pickup/delivery.\n",
    "        \n",
    "        Args:\n",
    "            agent (Agent): The agent to move\n",
    "            action (str): The action to take (\"up\", \"down\", \"left\", \"right\")\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (reward, info) where info contains collision and delivery status\n",
    "        \"\"\"\n",
    "        x, y = agent.position\n",
    "        possible_moves = {\n",
    "            \"up\": (-1, 0),\n",
    "            \"down\": (1, 0),\n",
    "            \"left\": (0, -1),\n",
    "            \"right\": (0, 1)\n",
    "        }\n",
    "        dx, dy = possible_moves[action]\n",
    "        new_x, new_y = x + dx, y + dy\n",
    "        new_pos = (new_x, new_y)\n",
    "\n",
    "        # Initialize reward and info\n",
    "        reward = self.rewards['base_step_cost']  # Base step cost\n",
    "        info = {\"valid\": True, \"collision\": False, \"delivery\": False}\n",
    "\n",
    "        # Wall collision - still prevent these as they're out of bounds\n",
    "        if not (0 <= new_x < self.grid_size and 0 <= new_y < self.grid_size):\n",
    "            return -1, {\"valid\": False, \"collision\": False, \"delivery\": False}\n",
    "\n",
    "        # Calculate Manhattan distance to target\n",
    "        target_x, target_y = self.target_position if agent.has_package else self.package_position\n",
    "        current_dist = abs(x - target_x) + abs(y - target_y)\n",
    "        new_dist = abs(new_x - target_x) + abs(new_y - target_y)\n",
    "\n",
    "        # Heuristic 1: Reward moving closer to target\n",
    "        if new_dist < current_dist:\n",
    "            reward += self.rewards['closer_reward']  # Small reward for moving closer\n",
    "        elif new_dist > current_dist:\n",
    "            reward += self.rewards['away_penalty']  # Small penalty for moving away\n",
    "\n",
    "        # Head-on collision - allow but penalize if needed\n",
    "        collision = self.is_head_on_collision(agent, new_pos)\n",
    "        if collision:\n",
    "            reward += self.rewards['collision_penalty']  # Currently 0, can be adjusted later\n",
    "            info[\"collision\"] = True\n",
    "\n",
    "        # Move agent\n",
    "        agent.position = new_pos\n",
    "\n",
    "        # Handle package pickup at A\n",
    "        if agent.position == self.package_position and not agent.has_package:\n",
    "            agent.has_package = True\n",
    "            reward += self.rewards['pickup_reward']  # Reward for pickup\n",
    "\n",
    "        # Handle package delivery at B\n",
    "        if agent.position == self.target_position and agent.has_package:\n",
    "            agent.has_package = False\n",
    "            reward += self.rewards['delivery_reward']  # Reward for successful delivery\n",
    "            info[\"delivery\"] = True\n",
    "\n",
    "        self.update_grid()\n",
    "        return reward, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one step in the environment.\n",
    "        The current agent takes an action, and the environment updates.\n",
    "        \n",
    "        Args:\n",
    "            action (str): The action for the current agent to take\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (observation, reward, info)\n",
    "        \"\"\"\n",
    "        current_agent = self.agents[self.current_agent_idx]\n",
    "        reward, info = self.move_agent(current_agent, action)\n",
    "        self.current_agent_idx = (self.current_agent_idx + 1) % self.num_agents\n",
    "        next_agent = self.agents[self.current_agent_idx]\n",
    "        observation = {\n",
    "            'state': next_agent.get_state(self.package_position, self.target_position)\n",
    "        }\n",
    "        return observation, reward, info\n",
    "\n",
    "    def update_grid(self):\n",
    "        \"\"\"\n",
    "        Update the grid representation with current agent positions and package status.\n",
    "        \"\"\"\n",
    "        self.grid = np.zeros((self.grid_size, self.grid_size), dtype=object)\n",
    "        self.grid[self.package_position] = \"A\"\n",
    "        self.grid[self.target_position] = \"B\"\n",
    "        for agent in self.agents:\n",
    "            x, y = agent.position\n",
    "            if agent.has_package:\n",
    "                self.grid[x, y] = f\"AG{agent.agent_id}(A)\"\n",
    "            else:\n",
    "                self.grid[x, y] = f\"AG{agent.agent_id}\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to a new random state.\n",
    "        Randomly places A and B, and resets agent positions.\n",
    "        \"\"\"\n",
    "        # Randomly place A and B\n",
    "        positions = self.get_random_positions(2)\n",
    "        self.package_position = positions[0]\n",
    "        self.target_position = positions[1]\n",
    "        \n",
    "        # Reset agents\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            start_position = random.choice([self.package_position, self.target_position])\n",
    "            \n",
    "            agent.position = start_position\n",
    "            agent.has_package = (start_position == self.package_position)\n",
    "        \n",
    "        self.update_grid()\n",
    "        self.current_agent_idx = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how get_opposite_agents is defined but not used. This is because we are using only one agent during training so there is no need to check this. However, the method stays within the class in case there is a modification to the training later and its use is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning Training Process\n",
    "\n",
    "Something important to highlight here, is how the relation between exploration/exploitation (epsilon start, end and decay values) and the rest of the hyperparameters were chosen. Initially the idea was to be conservative, with a low epsilon from the start (0.3) and to reach the minimum value (0.1) at approximately 1.000.000 steps (66% of the training). For this, we simply did some basic algebra knowing that E_min = E_start * E_decay^N with N as the number of steps, and found E_decay which after this calculation was 0.9999989. The learning rate was initially set to 0.1 because it is a commonly used setting in Q learning which allows our agent to learn without changing the Q values too aggresive or too slow. The discount factor was initially set as 0.99 since our agents need to learn a path so it is important  for them to prioritize future rewards.\n",
    "\n",
    "After some iterations with these initial values, the only one that changed was E_decay = 0.999999 which gave us the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentQLearning:\n",
    "    \"\"\"\n",
    "    Implements Q-learning for multiple agents in the delivery environment.\n",
    "    \n",
    "    This class manages the Q-table and implements the Q-learning algorithm for training\n",
    "    agents. It handles state representation, action selection using epsilon-greedy policy,\n",
    "    and Q-value updates. The Q-table is shared among all agents, allowing them to learn\n",
    "    from each other's experiences.\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_size, num_agents, \n",
    "                 learning_rate=0.1,\n",
    "                 discount_factor=0.99,\n",
    "                 epsilon_start=0.3,\n",
    "                 epsilon_min=0.1,\n",
    "                 epsilon_decay=0.999999):\n",
    "        \"\"\"\n",
    "        Initialize Q-learning for multiple agents.\n",
    "        \n",
    "        Args:\n",
    "            grid_size (int): Size of the grid\n",
    "            num_agents (int): Number of agents\n",
    "            learning_rate (float): Alpha parameter for Q-learning\n",
    "            discount_factor (float): Gamma parameter for Q-learning\n",
    "            epsilon_start (float): Initial exploration rate\n",
    "            epsilon_min (float): Minimum exploration rate\n",
    "            epsilon_decay (float): Rate at which epsilon decays\n",
    "        \"\"\"\n",
    "        self.alpha = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon = epsilon_start\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        \n",
    "        self.grid_size = grid_size\n",
    "        self.num_agents = num_agents\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.action_indices = {a: i for i, a in enumerate(self.actions)}\n",
    "        \n",
    "        # Single shared Q-table: (agent_x, agent_y, package_x, package_y, target_x, target_y, has_package, action)\n",
    "        self.q_table = np.zeros((grid_size, grid_size, grid_size, grid_size, grid_size, grid_size, 2, 4))\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay epsilon and ensure it doesn't go below minimum.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def get_action(self, agent, state):\n",
    "        \"\"\"\n",
    "        Get the next action for an agent using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            agent (Agent): The agent to get action for\n",
    "            state (tuple): Current state of the agent\n",
    "            \n",
    "        Returns:\n",
    "            str: The selected action\n",
    "        \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        x, y, px, py, tx, ty, has_package = state\n",
    "        has_package = int(has_package)\n",
    "        q_values = self.q_table[x, y, px, py, tx, ty, has_package, :]\n",
    "        best_action_idx = np.argmax(q_values)\n",
    "        return self.actions[best_action_idx]\n",
    "\n",
    "    def update(self, agent, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Update Q-values using Q-learning update rule.\n",
    "        \n",
    "        Args:\n",
    "            agent (Agent): The agent being updated\n",
    "            state (tuple): Current state\n",
    "            action (str): Action taken\n",
    "            reward (float): Reward received\n",
    "            next_state (tuple): Next state\n",
    "        \"\"\"\n",
    "        action_indices = self.action_indices\n",
    "        x, y, px, py, tx, ty, has_package = state\n",
    "        has_package = int(has_package)\n",
    "        nx, ny, npx, npy, ntx, nty, nhas_package = next_state\n",
    "        nhas_package = int(nhas_package)\n",
    "        a_idx = action_indices[action]\n",
    "        current_q = self.q_table[x, y, px, py, tx, ty, has_package, a_idx]\n",
    "        max_next_q = np.max(self.q_table[nx, ny, npx, npy, ntx, nty, nhas_package, :])\n",
    "        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[x, y, px, py, tx, ty, has_package, a_idx] = new_q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train single Agent loop\n",
    "\n",
    "This returns the q values table along with the history of rewards, deliveries, collisions (as mentioned many times, in this training always 0 but still defined in case the training is modified and improved later on) and epsilon recorded every defined progress interval, which in this case is 10000 steps. As mentioned in the methodology section, the environment is reset every 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_agent(env, q_learning, steps):\n",
    "    \"\"\"\n",
    "    Train a single agent without collision tracking.\n",
    "    \n",
    "    Args:\n",
    "        env (MultiAgentEnvironment): The environment to train in\n",
    "        q_learning (MultiAgentQLearning): The Q-learning model\n",
    "        steps (int): Number of training steps\n",
    "        \n",
    "    Returns:\n",
    "        dict: Training metrics and results\n",
    "    \"\"\"\n",
    "    total_steps = 0\n",
    "    deliveries = 0\n",
    "    rewards = 0\n",
    "    \n",
    "    rewards_history = [0]\n",
    "    deliveries_history = [0]\n",
    "    collisions_history = [0]\n",
    "    epsilon_history = [q_learning.epsilon]\n",
    "    progress_interval = 10000\n",
    "    \n",
    "    while total_steps < steps:\n",
    "        if total_steps % 100 == 0:\n",
    "            env.reset()\n",
    "            \n",
    "        current_agent = env.agents[env.current_agent_idx]\n",
    "        state = current_agent.get_state(env.package_position, env.target_position)\n",
    "        action = q_learning.get_action(current_agent, state)\n",
    "        next_observation, reward, info = env.step(action)\n",
    "        q_learning.update(current_agent, state, action, reward, next_observation['state'])\n",
    "        \n",
    "        q_learning.decay_epsilon()\n",
    "        rewards += reward\n",
    "        if info[\"delivery\"]:\n",
    "            deliveries += 1\n",
    "        total_steps += 1\n",
    "        \n",
    "        if total_steps % progress_interval == 0:\n",
    "            print(f\"Steps: {total_steps}, Deliveries: {deliveries}\")\n",
    "            print(f\"Current epsilon: {q_learning.epsilon:.4f}\")\n",
    "            rewards_history.append(rewards)\n",
    "            deliveries_history.append(deliveries)\n",
    "            collisions_history.append(0)  # Single agent has no collisions\n",
    "            epsilon_history.append(q_learning.epsilon)\n",
    "    \n",
    "    return {\n",
    "        'rewards': rewards_history,\n",
    "        'deliveries': deliveries_history,\n",
    "        'collisions': collisions_history,\n",
    "        'epsilon_history': epsilon_history,\n",
    "        'q_learning': q_learning\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Here we create an environment, a Q Learning instance, and we input these two into our train single agent loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "    steps=1_500_000,\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.99,\n",
    "    epsilon_start=0.3,\n",
    "    epsilon_min=0.1,\n",
    "    epsilon_decay=0.999999,\n",
    "    rewards=REWARDS\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a single agent with Q-learning.\n",
    "    \n",
    "    Args:\n",
    "        steps (int): Number of training steps\n",
    "        learning_rate (float): Alpha parameter for Q-learning\n",
    "        discount_factor (float): Gamma parameter for Q-learning\n",
    "        epsilon_start (float): Initial exploration rate\n",
    "        epsilon_min (float): Minimum exploration rate\n",
    "        epsilon_decay (float): Rate at which epsilon decays\n",
    "        rewards (dict): Dictionary of reward values\n",
    "        \n",
    "    Returns:\n",
    "        dict: Training results and environment\n",
    "    \"\"\"\n",
    "    print(\"Starting training...\")\n",
    "    # Create environment and Q-learning agent\n",
    "    env = MultiAgentEnvironment(\n",
    "        grid_size=5, \n",
    "        num_agents=1,\n",
    "        rewards=rewards\n",
    "    )\n",
    "    q_learning = MultiAgentQLearning(\n",
    "        grid_size=5,\n",
    "        num_agents=1,\n",
    "        learning_rate=learning_rate,\n",
    "        discount_factor=discount_factor,\n",
    "        epsilon_start=epsilon_start,\n",
    "        epsilon_min=epsilon_min,\n",
    "        epsilon_decay=epsilon_decay\n",
    "    )\n",
    "    \n",
    "    # Train single agent\n",
    "    results = train_single_agent(env, q_learning, steps)\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'q_learning': q_learning,\n",
    "        'env': env\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_agent(\n",
    "        steps=1_500_000,\n",
    "        learning_rate=0.1,\n",
    "        discount_factor=0.99,\n",
    "        epsilon_start=0.3,\n",
    "        epsilon_min=0.1,\n",
    "        epsilon_decay=0.999999,\n",
    "        rewards=REWARDS\n",
    "    )\n",
    "\n",
    "q_learning = results['q_learning']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training metrics plotting\n",
    "\n",
    "Using the return of the train_single_agent method, we plot the rewards, deliveries and collisions over time to visualize how the trained agent is actually improving over time in relation to these metrics.\n",
    "\n",
    "We show for each metric the cumulative value over time, and also the value of each metric per step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(metrics, progress_interval=10000):\n",
    "    \"\"\"\n",
    "    Plot the training metrics over time.\n",
    "\n",
    "    Args:\n",
    "        metrics (dict): Dictionary containing training metrics:\n",
    "                        - 'rewards': List of cumulative rewards\n",
    "                        - 'deliveries': List of cumulative deliveries\n",
    "                        - 'collisions': List of cumulative collisions\n",
    "        progress_interval (int): Number of steps between each metric record\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(15, 12))  # 3 rows, 2 columns\n",
    "\n",
    "    # Create x-axis values representing environment steps\n",
    "    x = [i * progress_interval for i in range(len(metrics['rewards']))]\n",
    "\n",
    "    # Left column: Cumulative metrics\n",
    "    axs[0, 0].plot(x, metrics['rewards'])\n",
    "    axs[0, 0].set_title('Cumulative Reward')\n",
    "    axs[0, 0].set_xlabel('Environment Steps')\n",
    "    axs[0, 0].set_ylabel('Reward')\n",
    "\n",
    "    axs[1, 0].plot(x, metrics['deliveries'])\n",
    "    axs[1, 0].set_title('Cumulative Deliveries')\n",
    "    axs[1, 0].set_xlabel('Environment Steps')\n",
    "    axs[1, 0].set_ylabel('Number of Deliveries')\n",
    "\n",
    "    axs[2, 0].plot(x, metrics['collisions'])\n",
    "    axs[2, 0].set_title('Cumulative Collisions')\n",
    "    axs[2, 0].set_xlabel('Environment Steps')\n",
    "    axs[2, 0].set_ylabel('Number of Collisions')\n",
    "\n",
    "    # Right column: Rate metrics (per interval)\n",
    "    reward_rates = np.diff([0] + metrics['rewards']) / progress_interval\n",
    "    delivery_rates = np.diff([0] + metrics['deliveries']) / progress_interval\n",
    "    collision_rates = np.diff([0] + metrics['collisions']) / progress_interval\n",
    "\n",
    "    axs[0, 1].plot(x, reward_rates)\n",
    "    axs[0, 1].set_title('Reward per step')\n",
    "    axs[0, 1].set_xlabel('Environment Steps')\n",
    "    axs[0, 1].set_ylabel('Avg Reward per step')\n",
    "\n",
    "    axs[1, 1].plot(x, delivery_rates)\n",
    "    axs[1, 1].set_title('Deliveries per step')\n",
    "    axs[1, 1].set_xlabel('Environment Steps')\n",
    "    axs[1, 1].set_ylabel('Deliveries per step')\n",
    "\n",
    "    axs[2, 1].plot(x, collision_rates)\n",
    "    axs[2, 1].set_title('Collisions per step')\n",
    "    axs[2, 1].set_xlabel('Environment Steps')\n",
    "    axs[2, 1].set_ylabel('Collisions per step')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training metrics plotting implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_metrics(results['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization, testing and final performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "In order to see the behaviour of our agents, we make a visualization of the grid, which will further allow to exemplify any desired scenario, with the purposes of both confirming that our agents actually act as expected and debug certain scenarios of interest. Our grid visualization always shows the number of deliveries and collisions for the scenario in process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentVisualizer:\n",
    "    \"\"\"\n",
    "    Handles the visualization of the multi-agent environment.\n",
    "    \n",
    "    This class provides methods to render the current state of the environment,\n",
    "    including the grid, agents, package pickup point (A), and delivery point (B).\n",
    "    It uses different colors to distinguish between agents with and without packages,\n",
    "    and provides options for displaying step-by-step visualization or saving images.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Initialize the visualizer with an environment.\n",
    "        \n",
    "        Args:\n",
    "            env (MultiAgentEnvironment): The grid world environment to visualize\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.colors = {\n",
    "            \"agent_to_b\": \"blue\",      # Agents moving towards B\n",
    "            \"agent_to_a\": \"green\",     # Agents moving towards A\n",
    "            \"package\": \"yellow\",       # Location A\n",
    "            \"target\": \"red\",          # Location B\n",
    "        }\n",
    "        \n",
    "    def render(self, title=None, show=True, save_path=None):\n",
    "        \"\"\"\n",
    "        Render the current state of the environment.\n",
    "        \n",
    "        Args:\n",
    "            title (str, optional): Title for the plot\n",
    "            show (bool): Whether to display the plot\n",
    "            save_path (str, optional): Path to save the plot image\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib.figure.Figure: The generated figure\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(7, 7))\n",
    "        \n",
    "        # Set plot limits\n",
    "        ax.set_xlim(-0.5, self.env.grid_size - 0.5)\n",
    "        ax.set_ylim(-0.5, self.env.grid_size - 0.5)\n",
    "        \n",
    "        # Draw grid lines\n",
    "        for i in range(self.env.grid_size + 1):\n",
    "            ax.axhline(i - 0.5, color='black', linestyle='-', alpha=0.2)\n",
    "            ax.axvline(i - 0.5, color='black', linestyle='-', alpha=0.2)\n",
    "        \n",
    "        # Draw package location (A)\n",
    "        x, y = self.env.package_position\n",
    "        package = patches.Rectangle((y - 0.4, x - 0.4), 0.8, 0.8, \n",
    "                                color=self.colors[\"package\"], alpha=0.7)\n",
    "        ax.add_patch(package)\n",
    "        ax.text(y, x, \"A\", ha='center', va='center', fontsize=12)\n",
    "        \n",
    "        # Draw target location (B)\n",
    "        x, y = self.env.target_position\n",
    "        target = patches.Rectangle((y - 0.4, x - 0.4), 0.8, 0.8, \n",
    "                                color=self.colors[\"target\"], alpha=0.7)\n",
    "        ax.add_patch(target)\n",
    "        ax.text(y, x, \"B\", ha='center', va='center', fontsize=12)\n",
    "        \n",
    "        # Draw agents\n",
    "        for agent in self.env.agents:\n",
    "            x, y = agent.position\n",
    "            # Choose color based package status\n",
    "            color = self.colors[\"agent_to_b\"] if agent.has_package else self.colors[\"agent_to_a\"]\n",
    "                \n",
    "            agent_circle = patches.Circle((y, x), 0.3, color=color, alpha=0.8)\n",
    "            ax.add_patch(agent_circle)\n",
    "            label = f\"{agent.agent_id}\"\n",
    "            ax.text(y, x, label, ha='center', va='center', fontsize=10, color='white')\n",
    "        \n",
    "        if title:\n",
    "            ax.set_title(title)\n",
    "        ax.set_xlabel(\"Column\")\n",
    "        ax.set_ylabel(\"Row\")\n",
    "        \n",
    "        ax.set_xticks(range(self.env.grid_size))\n",
    "        ax.set_yticks(range(self.env.grid_size))\n",
    "        \n",
    "        # Invert y-axis to match grid coordinates (0,0 at top-left)\n",
    "        ax.invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        \n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "    \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "This function allows us to test our agents visualizing their behaviour, where we can define the number of tests, the number of steps for each test, whether we want to show all the steps or define an interval of steps in which the test will show us the grid world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trained_agents(\n",
    "    env, q_learning, num_tests=10, steps_per_test=200, render=True, render_delay=0.5, \n",
    "    show_all_steps=False, policy=\"trained\", render_interval=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Test the Q-learning or random agents in the multi-agent environment and collect performance metrics.\n",
    "\n",
    "    Args:\n",
    "        env (MultiAgentEnvironment): The environment to test in (should have 4 agents)\n",
    "        q_learning (MultiAgentQLearning): The trained Q-learning model\n",
    "        num_tests (int): Number of test scenarios to run\n",
    "        steps_per_test (int): Maximum number of steps per test scenario\n",
    "        render (bool): Whether to render the environment visually\n",
    "        render_delay (float): Delay between renders in seconds\n",
    "        show_all_steps (bool): If True, renders every step; if False, renders every render_interval steps\n",
    "        policy (str): \"trained\" (Q-learning) or \"random\" (naive)\n",
    "        render_interval (int): Number of steps between renders when show_all_steps is False\n",
    "\n",
    "    Returns:\n",
    "        dict: Test metrics including:\n",
    "            - deliveries: List of deliveries completed in each test\n",
    "            - collisions: List of collisions in each test\n",
    "            - rewards: List of total rewards in each test\n",
    "            - avg_deliveries: Average deliveries across all tests\n",
    "            - avg_collisions: Average collisions across all tests\n",
    "            - avg_reward: Average reward across all tests\n",
    "    \"\"\"\n",
    "    visualizer = MultiAgentVisualizer(env)\n",
    "    all_deliveries = []\n",
    "    all_collisions = []\n",
    "    all_rewards = []\n",
    "\n",
    "    # For per-step plots of the first test\n",
    "    first_test_deliveries = []\n",
    "    first_test_collisions = []\n",
    "    first_test_rewards = []\n",
    "\n",
    "    for test_num in range(num_tests):\n",
    "        print(f\"\\nTest {test_num + 1}/{num_tests}\")\n",
    "        test_deliveries = 0\n",
    "        test_collisions = 0\n",
    "        test_reward = 0\n",
    "        per_step_deliveries = []\n",
    "        per_step_collisions = []\n",
    "        per_step_rewards = []\n",
    "\n",
    "        # Initialize counters for each agent\n",
    "        agent_deliveries = [0] * env.num_agents\n",
    "        agent_collisions = [0] * env.num_agents\n",
    "        agent_rewards = [0] * env.num_agents\n",
    "\n",
    "        for step in range(steps_per_test):\n",
    "            # Get the current agent and its state\n",
    "            current_agent = env.agents[env.current_agent_idx]\n",
    "            state = current_agent.get_state(env.package_position, env.target_position)\n",
    "            \n",
    "            # Action selection\n",
    "            if policy == \"trained\":\n",
    "                action = q_learning.get_action(current_agent, state)\n",
    "            elif policy == \"random\":\n",
    "                action = random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
    "            else:\n",
    "                raise ValueError(\"policy must be 'trained' or 'random'\")\n",
    "            \n",
    "            # Execute the action in the environment\n",
    "            next_observation, reward, info = env.step(action)\n",
    "            \n",
    "            # Update metrics for the current agent\n",
    "            agent_rewards[env.current_agent_idx] += reward\n",
    "            if info[\"delivery\"]:\n",
    "                agent_deliveries[env.current_agent_idx] += 1\n",
    "                test_deliveries += 1\n",
    "            if info[\"collision\"]:\n",
    "                agent_collisions[env.current_agent_idx] += 1\n",
    "                test_collisions += 1\n",
    "            test_reward += reward\n",
    "                \n",
    "            # Record per-step metrics for plotting\n",
    "            per_step_deliveries.append(test_deliveries)\n",
    "            per_step_collisions.append(test_collisions)\n",
    "            per_step_rewards.append(test_reward)\n",
    "            \n",
    "            # Render the environment based on settings\n",
    "            if show_all_steps:\n",
    "                visualizer.render(\n",
    "                    title=f\"Test {test_num + 1}, Step {step}\\n\"\n",
    "                          f\"Deliveries: {test_deliveries}, Collisions: {test_collisions}\\n\"\n",
    "                          f\"Agent {env.current_agent_idx} moving\",\n",
    "                    show=True\n",
    "                )\n",
    "            elif render and step % render_interval == 0:\n",
    "                visualizer.render(\n",
    "                    title=f\"Test {test_num + 1}, Step {step}\\n\"\n",
    "                          f\"Deliveries: {test_deliveries}, Collisions: {test_collisions}\\n\"\n",
    "                          f\"Agent {env.current_agent_idx} moving\",\n",
    "                    show=True\n",
    "                )\n",
    "                plt.pause(render_delay)\n",
    "                \n",
    "        # Store results for this test\n",
    "        all_deliveries.append(test_deliveries)\n",
    "        all_collisions.append(test_collisions)\n",
    "        all_rewards.append(test_reward)\n",
    "        \n",
    "        # Print per-agent results\n",
    "        print(f\"\\nTest {test_num + 1} Results:\")\n",
    "        for agent_idx in range(env.num_agents):\n",
    "            print(f\"Agent {agent_idx}: Deliveries={agent_deliveries[agent_idx]}, \"\n",
    "                  f\"Collisions={agent_collisions[agent_idx]}, \"\n",
    "                  f\"Reward={agent_rewards[agent_idx]:.2f}\")\n",
    "        print(f\"Total: Deliveries={test_deliveries}, Collisions={test_collisions}, Reward={test_reward:.2f}\")\n",
    "\n",
    "        # Save per-step metrics for the first test for detailed analysis\n",
    "        if test_num == 0:\n",
    "            first_test_deliveries = per_step_deliveries\n",
    "            first_test_collisions = per_step_collisions\n",
    "            first_test_rewards = per_step_rewards\n",
    "\n",
    "        # Reset environment for next test\n",
    "        env.reset()\n",
    "\n",
    "    # Calculate aggregate statistics\n",
    "    avg_deliveries = np.mean(all_deliveries)\n",
    "    avg_collisions = np.mean(all_collisions)\n",
    "    avg_reward = np.mean(all_rewards)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nOverall Test Results:\")\n",
    "    print(f\"Average Deliveries per Test: {avg_deliveries:.2f}\")\n",
    "    print(f\"Average Collisions per Test: {avg_collisions:.2f}\")\n",
    "    print(f\"Average Reward per Test: {avg_reward:.2f}\")\n",
    "\n",
    "    # Plot test results across all tests\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Deliveries plot\n",
    "    ax1.bar(range(num_tests), all_deliveries)\n",
    "    ax1.set_title('Deliveries per Test (4 Agents)')\n",
    "    ax1.set_xlabel('Test Number')\n",
    "    ax1.set_ylabel('Number of Deliveries')\n",
    "    ax1.axhline(y=avg_deliveries, color='r', linestyle='--', label='Average')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Collisions plot\n",
    "    ax2.bar(range(num_tests), all_collisions)\n",
    "    ax2.set_title('Collisions per Test (4 Agents)')\n",
    "    ax2.set_xlabel('Test Number')\n",
    "    ax2.set_ylabel('Number of Collisions')\n",
    "    ax2.axhline(y=avg_collisions, color='r', linestyle='--', label='Average')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Rewards plot\n",
    "    ax3.bar(range(num_tests), all_rewards)\n",
    "    ax3.set_title('Total Reward per Test (4 Agents)')\n",
    "    ax3.set_xlabel('Test Number')\n",
    "    ax3.set_ylabel('Reward')\n",
    "    ax3.axhline(y=avg_reward, color='r', linestyle='--', label='Average')\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot detailed per-step metrics for the first test\n",
    "    if len(first_test_deliveries) > 0:\n",
    "        steps = list(range(steps_per_test))\n",
    "        fig, axs = plt.subplots(3, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Left column: Cumulative metrics\n",
    "        axs[0, 0].plot(steps, first_test_rewards)\n",
    "        axs[0, 0].set_title('Cumulative Reward (First Test, 4 Agents)')\n",
    "        axs[0, 0].set_xlabel('Step')\n",
    "        axs[0, 0].set_ylabel('Total Reward')\n",
    "        \n",
    "        axs[1, 0].plot(steps, first_test_deliveries)\n",
    "        axs[1, 0].set_title('Cumulative Deliveries (First Test, 4 Agents)')\n",
    "        axs[1, 0].set_xlabel('Step')\n",
    "        axs[1, 0].set_ylabel('Total Deliveries')\n",
    "        \n",
    "        axs[2, 0].plot(steps, first_test_collisions)\n",
    "        axs[2, 0].set_title('Cumulative Collisions (First Test, 4 Agents)')\n",
    "        axs[2, 0].set_xlabel('Step')\n",
    "        axs[2, 0].set_ylabel('Total Collisions')\n",
    "        \n",
    "        # Right column: Per-step rates\n",
    "        reward_rate = np.diff([0] + first_test_rewards)\n",
    "        delivery_rate = np.diff([0] + first_test_deliveries)\n",
    "        collision_rate = np.diff([0] + first_test_collisions)\n",
    "        \n",
    "        axs[0, 1].plot(steps, reward_rate)\n",
    "        axs[0, 1].set_title('Reward per Step (First Test, 4 Agents)')\n",
    "        axs[0, 1].set_xlabel('Step')\n",
    "        axs[0, 1].set_ylabel('Step Reward')\n",
    "        \n",
    "        axs[1, 1].plot(steps, delivery_rate)\n",
    "        axs[1, 1].set_title('Delivery per Step (First Test, 4 Agents)')\n",
    "        axs[1, 1].set_xlabel('Step')\n",
    "        axs[1, 1].set_ylabel('Step Deliveries')\n",
    "        \n",
    "        axs[2, 1].plot(steps, collision_rate)\n",
    "        axs[2, 1].set_title('Collision per Step (First Test, 4 Agents)')\n",
    "        axs[2, 1].set_xlabel('Step')\n",
    "        axs[2, 1].set_ylabel('Step Collisions')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        'deliveries': all_deliveries,\n",
    "        'collisions': all_collisions,\n",
    "        'rewards': all_rewards,\n",
    "        'avg_deliveries': avg_deliveries,\n",
    "        'avg_collisions': avg_collisions,\n",
    "        'avg_reward': avg_reward\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing implementation\n",
    "\n",
    "We will different test cases listed below:\n",
    "\n",
    "- 5 tests with 200 steps per tests where we visualize the grid every 5 steps (to not overload the view, but can be changed if desired) for both random and trained agents to visualize how different they behave in different scenarios.\n",
    "\n",
    "- 3 tests with 50 steps each where we visualize all steps for our trained agents, just to visualize the complete behaviour of each single agent in each step in different scenarios.\n",
    "\n",
    "- A debug test where we visualize all steps for a single test where we define A and B positions. The objective of this test is to debug and visualize the behaviour of our agents in specific scenarios we are interested in.\n",
    "\n",
    "*Note: Visualization of all these scenarios can be a lot, so in case not eveything is wanted to be visualized the notebook user can interrupt the execution and \"Clear all ouputs\" of the corresponding cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = MultiAgentEnvironment(grid_size=5, num_agents=4, rewards=REWARDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random agents - 5 random tests - visualization every 5 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_results = test_trained_agents(test_env, q_learning, num_tests=5, steps_per_test=200, \n",
    "                                       render=True, render_delay=0.5, policy=\"random\", render_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trained agents - 5 random tests - visualization every 5 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_results = test_trained_agents(test_env, q_learning, num_tests=5, steps_per_test=200, \n",
    "                                        render=True, render_delay=0.5, policy=\"trained\", render_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trained agents - 3 random tests - visualization in all steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_num in range(3):\n",
    "        print(f\"\\nDetailed Test {test_num + 1}/3\")\n",
    "        detailed_env = MultiAgentEnvironment(grid_size=5, num_agents=4, rewards=REWARDS)\n",
    "        detailed_results = test_trained_agents(\n",
    "            detailed_env,\n",
    "            q_learning,\n",
    "            num_tests=1,\n",
    "            steps_per_test=50,\n",
    "            render=True,\n",
    "            render_delay=0.5,\n",
    "            show_all_steps=True,\n",
    "            policy=\"trained\",\n",
    "            render_interval=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specific Scenario test (Debug)\n",
    "\n",
    "Change package_position and target_position when creating the environment below to the desired A and B positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_env = MultiAgentEnvironment(\n",
    "        grid_size=5, \n",
    "        num_agents=4,\n",
    "        package_position=(4,4),\n",
    "        target_position=(3,0),\n",
    "        rewards=REWARDS\n",
    "    )\n",
    "\n",
    "specific_results = test_trained_agents(\n",
    "        specific_env, \n",
    "        q_learning, \n",
    "        num_tests=1, \n",
    "        steps_per_test=200, \n",
    "        render=True, \n",
    "        render_delay=0.5, \n",
    "        show_all_steps=True,\n",
    "        policy=\"trained\",\n",
    "        render_interval=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Performance\n",
    "\n",
    "Here we evaluate the final performance of a group of agents (random or trained) in a test that simulates all possible scenarios (all posible combinations of A and B, with each of the 4 agents starting at A or B), which gives us 9600 different scenarios. The target our agents is to succeed in 75% of them.\n",
    "\n",
    "Let's remember the definition of \"succeed\":\n",
    "\n",
    "- The agents succeed in a scenario if all of them manage to get to A (starting from B), take the package and then deliver the package at B without any collisions in no more than 25 steps (per agent). Since some agents can also start at A, we give them some time to get to B (also 25 steps since it is not specified by the assignment requirements) and then we start the timer of 25 steps to complete their \"journey\".\n",
    "\n",
    "We will evaluate both random (naive) and trained agents and will record the number of successful scenarios, the number of scenarios failed by collisions, the number of scenarios failed due to timeouts, and the rewards obtained. Then we will calculate the overall success rate, which is expected to be over 75%, and will print this along with the other recorded metrics.\n",
    "\n",
    "After this, we will plot these metrics with the x axis being the scenario (from 1 to 9600).\n",
    "\n",
    "Finally, it will also print a table with columns as metric, random agents, trained agents and improvement percentage of trained compared to random where we will compare success rate, total deliveries, scenarios with collisions, scenarios with timeouts and total rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiagent_final_performance_qtable(\n",
    "    env_class, q_learning, grid_size=5, num_agents=4, max_agent_steps=25, plot=True, policy=\"trained\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a Q-table policy in a multi-agent environment.\n",
    "    Records and plots metrics for comparison.\n",
    "\n",
    "    Args:\n",
    "        env_class: The environment class to instantiate\n",
    "        q_learning: The trained MultiAgentQLearning object (None for random policy)\n",
    "        grid_size: Size of the grid environment (default: 5x5)\n",
    "        num_agents: Number of agents in the environment (default: 4)\n",
    "        max_agent_steps: Maximum steps allowed after visiting position B (default: 25)\n",
    "        plot: Whether to plot the results at the end\n",
    "        policy: \"trained\" for Q-learning or \"random\" for random actions\n",
    "\n",
    "    Returns:\n",
    "        success_rates: List of success rates for each agent\n",
    "        metrics: Dict of per-scenario metrics\n",
    "    \"\"\"\n",
    "    positions = [(x, y) for x in range(grid_size) for y in range(grid_size)]\n",
    "    total_scenarios = 0\n",
    "    successful_scenarios = 0  # Count scenarios where ALL agents succeed\n",
    "\n",
    "    # Basic counters\n",
    "    total_deliveries = 0\n",
    "    total_collisions = 0 \n",
    "    total_timeouts = 0\n",
    "    total_rewards = 0\n",
    "\n",
    "    # For plotting\n",
    "    scenario_success = []  # 1 for success, 0 for failure\n",
    "    scenario_deliveries = []  # Number of deliveries in each scenario\n",
    "    scenario_collisions = []  # 1 if scenario had collision, 0 otherwise\n",
    "    scenario_timeouts = []  # 1 if scenario had timeout, 0 otherwise\n",
    "    scenario_rewards = []  # Total rewards in each scenario\n",
    "\n",
    "    scenario_count = 0\n",
    "    for a_pos in positions:\n",
    "        for b_pos in positions:\n",
    "            if a_pos == b_pos:\n",
    "                continue\n",
    "            for agent_starts in itertools.product([a_pos, b_pos], repeat=num_agents):\n",
    "                env = env_class(grid_size=grid_size, num_agents=num_agents)\n",
    "                # Manually set A and B\n",
    "                env.package_position = a_pos\n",
    "                env.target_position = b_pos\n",
    "                # Manually set agent positions and has_package\n",
    "                for i, agent in enumerate(env.agents):\n",
    "                    agent.position = agent_starts[i]\n",
    "                    agent.has_package = (agent_starts[i] == a_pos)\n",
    "\n",
    "                pre_timer = [None if agent_starts[i] == b_pos else 0 for i in range(num_agents)]\n",
    "                timer_started = [False if agent_starts[i] == a_pos else True for i in range(num_agents)]\n",
    "                steps_since_b = [0] * num_agents\n",
    "                delivered = [False] * num_agents\n",
    "                collided = [False] * num_agents\n",
    "                timed_out = [False] * num_agents\n",
    "                scenario_reward = 0\n",
    "                scenario_delivery_count = 0\n",
    "                scenario_had_collision = False\n",
    "                scenario_had_timeout = False\n",
    "\n",
    "                while any(\n",
    "                    (not delivered[i] and not collided[i] and not timed_out[i] and\n",
    "                     ((pre_timer[i] is None and (not timer_started[i] or steps_since_b[i] < max_agent_steps)) or\n",
    "                      (pre_timer[i] is not None and pre_timer[i] < max_agent_steps)))\n",
    "                    for i in range(num_agents)\n",
    "                ):\n",
    "                    current_agent_idx = env.current_agent_idx\n",
    "                    agent = env.agents[current_agent_idx]\n",
    "\n",
    "                    # Action selection based on policy\n",
    "                    if policy == \"random\":\n",
    "                        action = random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
    "                    else:  # trained policy\n",
    "                        state = agent.get_state(env.package_position, env.target_position)\n",
    "                        x, y, px, py, tx, ty, has_package = state\n",
    "                        has_package = int(has_package)\n",
    "                        q_values = q_learning.q_table[x, y, px, py, tx, ty, has_package, :]\n",
    "                        action_idx = np.argmax(q_values)\n",
    "                        action_map = {0: \"up\", 1: \"down\", 2: \"left\", 3: \"right\"}\n",
    "                        action = action_map[action_idx]\n",
    "\n",
    "                    next_observation, reward, info = env.step(action)\n",
    "                    scenario_reward += reward\n",
    "                    total_rewards += reward\n",
    "\n",
    "                    # Only update status if not already done\n",
    "                    if not (delivered[current_agent_idx] or collided[current_agent_idx] or timed_out[current_agent_idx]):\n",
    "                        if pre_timer[current_agent_idx] is not None:\n",
    "                            if tuple(agent.position) == b_pos:\n",
    "                                pre_timer[current_agent_idx] = None\n",
    "                                timer_started[current_agent_idx] = True\n",
    "                                steps_since_b[current_agent_idx] = 0\n",
    "                            else:\n",
    "                                pre_timer[current_agent_idx] += 1\n",
    "                                if pre_timer[current_agent_idx] >= max_agent_steps:\n",
    "                                    timed_out[current_agent_idx] = True\n",
    "                                    scenario_had_timeout = True\n",
    "                                    total_timeouts += 1\n",
    "                        elif timer_started[current_agent_idx]:\n",
    "                            steps_since_b[current_agent_idx] += 1\n",
    "                            if steps_since_b[current_agent_idx] >= max_agent_steps:\n",
    "                                timed_out[current_agent_idx] = True\n",
    "                                scenario_had_timeout = True\n",
    "                                total_timeouts += 1\n",
    "                        elif tuple(agent.position) == b_pos:\n",
    "                            timer_started[current_agent_idx] = True\n",
    "                            steps_since_b[current_agent_idx] = 0\n",
    "\n",
    "                        if info[\"collision\"]:\n",
    "                            collided[current_agent_idx] = True\n",
    "                            scenario_had_collision = True\n",
    "                        if info[\"delivery\"]:\n",
    "                            delivered[current_agent_idx] = True\n",
    "                            total_deliveries += 1\n",
    "                            scenario_delivery_count += 1\n",
    "\n",
    "                # Count successful scenarios (all agents must succeed)\n",
    "                is_successful = all(delivered) and not any(collided) and not any(timed_out)\n",
    "                if is_successful:\n",
    "                    successful_scenarios += 1\n",
    "                    scenario_success.append(1)\n",
    "                else:\n",
    "                    scenario_success.append(0)\n",
    "\n",
    "                scenario_deliveries.append(scenario_delivery_count)\n",
    "                scenario_collisions.append(1 if scenario_had_collision else 0)\n",
    "                scenario_timeouts.append(1 if scenario_had_timeout else 0)\n",
    "                if scenario_had_collision:\n",
    "                    total_collisions += 1\n",
    "                scenario_rewards.append(scenario_reward)\n",
    "\n",
    "                total_scenarios += 1\n",
    "                scenario_count += 1\n",
    "                if scenario_count % 100 == 0:\n",
    "                    print(f\"Tested {scenario_count} scenarios...\")\n",
    "\n",
    "    # Calculate overall success rate (scenarios where ALL agents succeed)\n",
    "    overall_success_rate = successful_scenarios / total_scenarios if total_scenarios > 0 else 0\n",
    "    \n",
    "    print(\"\\nFinal Evaluation Results:\")\n",
    "    print(f\"Successful Scenarios: {successful_scenarios}/{total_scenarios}\")\n",
    "    print(f\"Overall Success Rate: {overall_success_rate*100:.2f}% (target: 75%)\")\n",
    "    print(f\"\\nTotal Counts:\")\n",
    "    print(f\"Total Deliveries: {total_deliveries}\")\n",
    "    print(f\"Scenarios with Collisions: {total_collisions}\")\n",
    "    print(f\"Scenarios with Timeouts: {total_timeouts}\")\n",
    "    print(f\"Total Rewards: {total_rewards}\")\n",
    "\n",
    "    # Plot metrics if requested\n",
    "    if plot:\n",
    "        scenarios = np.arange(total_scenarios)\n",
    "        fig, axs = plt.subplots(5, 1, figsize=(15, 25))\n",
    "        \n",
    "        # Success (1 or 0)\n",
    "        axs[0].plot(scenarios, scenario_success)\n",
    "        axs[0].set_title('Success per Scenario (1 = Success, 0 = Failure)')\n",
    "        axs[0].set_xlabel('Scenario')\n",
    "        axs[0].set_ylabel('Success')\n",
    "        axs[0].set_ylim(-0.1, 1.1)\n",
    "        \n",
    "        # Deliveries\n",
    "        axs[1].plot(scenarios, scenario_deliveries)\n",
    "        axs[1].set_title('Deliveries per Scenario')\n",
    "        axs[1].set_xlabel('Scenario')\n",
    "        axs[1].set_ylabel('Number of Deliveries')\n",
    "        \n",
    "        # Collisions\n",
    "        axs[2].plot(scenarios, scenario_collisions)\n",
    "        axs[2].set_title('Collisions per Scenario (1 = Had Collision, 0 = No Collision)')\n",
    "        axs[2].set_xlabel('Scenario')\n",
    "        axs[2].set_ylabel('Collision')\n",
    "        axs[2].set_ylim(-0.1, 1.1)\n",
    "        \n",
    "        # Timeouts\n",
    "        axs[3].plot(scenarios, scenario_timeouts)\n",
    "        axs[3].set_title('Timeouts per Scenario (1 = Had Timeout, 0 = No Timeout)')\n",
    "        axs[3].set_xlabel('Scenario')\n",
    "        axs[3].set_ylabel('Timeout')\n",
    "        axs[3].set_ylim(-0.1, 1.1)\n",
    "        \n",
    "        # Rewards\n",
    "        axs[4].plot(scenarios, scenario_rewards)\n",
    "        axs[4].set_title('Rewards per Scenario')\n",
    "        axs[4].set_xlabel('Scenario')\n",
    "        axs[4].set_ylabel('Total Reward')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    metrics = {\n",
    "        \"Success rate\": overall_success_rate,\n",
    "        \"Total deliveries\": total_deliveries,\n",
    "        \"Scenarios with collisions\": total_collisions,\n",
    "        \"Scenarios with timeouts\": total_timeouts,\n",
    "        \"Total rewards\": total_rewards\n",
    "    }\n",
    "    return [overall_success_rate] * num_agents, metrics  # Return same success rate for all agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Performance Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate random agents\n",
    "print(\"\\nEvaluating random agents...\")\n",
    "random_success_rates, random_metrics = evaluate_multiagent_final_performance_qtable(\n",
    "    MultiAgentEnvironment, None, grid_size=5, num_agents=4, policy=\"random\"\n",
    ")\n",
    "\n",
    "# Evaluate trained agents\n",
    "print(\"\\nEvaluating trained agents...\")\n",
    "trained_success_rates, trained_metrics = evaluate_multiagent_final_performance_qtable(\n",
    "    MultiAgentEnvironment, q_learning, grid_size=5, num_agents=4\n",
    ")\n",
    "\n",
    "# Compare evaluation results in a table\n",
    "print(\"\\nEvaluation Results Comparison:\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Metric':<30} {'Random Agents':<20} {'Trained Agents':<20} {'Improvement':<20}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Calculate improvements\n",
    "success_improvement = ((np.mean(trained_success_rates) - np.mean(random_success_rates)) / \n",
    "                        np.mean(random_success_rates) * 100)\n",
    "delivery_improvement = ((trained_metrics['Total deliveries'] - random_metrics['Total deliveries']) / \n",
    "                        random_metrics['Total deliveries'] * 100)\n",
    "collision_improvement = ((random_metrics['Scenarios with collisions'] - trained_metrics['Scenarios with collisions']) / \n",
    "                        random_metrics['Scenarios with collisions'] * 100)\n",
    "timeout_improvement = ((random_metrics['Scenarios with timeouts'] - trained_metrics['Scenarios with timeouts']) / \n",
    "                        random_metrics['Scenarios with timeouts'] * 100)\n",
    "reward_improvement = ((trained_metrics['Total rewards'] - random_metrics['Total rewards']) / \n",
    "                        random_metrics['Total rewards'] * 100)\n",
    "\n",
    "print(f\"{'Success Rate':<30} {np.mean(random_success_rates)*100:<20.2f}% {np.mean(trained_success_rates)*100:<20.2f}% {success_improvement:>19.1f}%\")\n",
    "print(f\"{'Total deliveries':<30} {random_metrics['Total deliveries']:<20.2f} {trained_metrics['Total deliveries']:<20.2f} {delivery_improvement:>19.1f}%\")\n",
    "print(f\"{'Scenarios with collisions':<30} {random_metrics['Scenarios with collisions']:<20.2f} {trained_metrics['Scenarios with collisions']:<20.2f} {collision_improvement:>19.1f}%\")\n",
    "print(f\"{'Scenarios with timeouts':<30} {random_metrics['Scenarios with timeouts']:<20.2f} {trained_metrics['Scenarios with timeouts']:<20.2f} {timeout_improvement:>19.1f}%\")\n",
    "print(f\"{'Total rewards':<30} {random_metrics['Total rewards']:<20.2f} {trained_metrics['Total rewards']:<20.2f} {reward_improvement:>19.1f}%\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training metrics plotting we can see, the rewards and the deliveries per step increase over time. We observe that we are reaching a value of between 0.15 and 0.2 deliveries per step, which means that every 5 to 7 steps there is a delivery. This is a good sign since for our desired performance, an agent should take approx 25 steps to make a delivery, which considering that we have 4 agents, could result in having a delivery in 100 steps, and we are way below this value.\n",
    "\n",
    "From the collisions graph we can see there are no collisions since as it has been mentioned many times before, the training is being done by a single agent, this 0 collisions value is definetely much less than the performance points target of 400."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests and Final Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final performance evaluation shows that the agents with no training (random-naive) have a succesful rate of less than 1%, while our trained agents have a sucess rate of more than 78% which meets the target of 75%.\n",
    "\n",
    "We definetely significantly improved in all the metrics by training our agents. It is interesting how the scenarios that are failing because of timeouts for our training agents is 0, which is the expected behaviour since the single agent training allowed them to learn effective paths between A and B, so the only reason why our agents fail is because they collide.\n",
    "\n",
    "What is most interesting out of everything is how our agents do not collide in more than 78% of the scenarios, when collisions were not considered during the training. This is a very interesting fact and after visualizing some cases and reasoning about it I found the reason.\n",
    "\n",
    "When visualizing the behaviour of the agents, agents from A to B in most cases would choose a different path than from B to A. The Q table in its last dimension has 4 possible values (4 actions: 0: up, 1: down, 2: left and 3: right), so when we do action_idx = np.argmax(q_values) for choosing the action it may change the returned action depending on the current position. This is because let's say agent \"0\" is in A (0,0) while agent \"1\" is in B (1,1), for agent \"0\" going right or down have the same best q-value, but since \"down\" is first in the q-table, it will tell the agent to go down, while agent \"1\" has the best q-value when going \"up\" or \"left\", but since \"up\" is first in the q-table, then it will tell the agent to take this direction. So basically the solution works well because of how the order of actions is defined, if i changed it to let's say (0: down, 2: left, 3:up, 4: right) then my agents would definetely always collide\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "From the last paragraph in the previous section we can infer why we still have around 20% of cases where the agents collide and many of them are when A and B are in the same row or column, because in this case there is indeed only one best path from A to B and it is the same that from B to A. The percentage of cases where A and B are aligned is 17%, which is close to our 20% of our failed cases, so definetely unless we add another training stage for these cases where the agents somehow learn to take different paths depending on the direction, we will not be able to reach more than 83% of success. Let's visualize some of these cases to prove that those are the cases where our agents our colliding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A and B in the same row (row 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_row_env = MultiAgentEnvironment(\n",
    "    grid_size=5,\n",
    "    num_agents=4,\n",
    "    package_position=(2,0),\n",
    "    target_position=(2,4),\n",
    "    rewards=REWARDS\n",
    ")\n",
    "same_row_results = test_trained_agents(\n",
    "    same_row_env,\n",
    "    q_learning,\n",
    "    num_tests=1,\n",
    "    steps_per_test=200,\n",
    "    render=True,\n",
    "    render_delay=0.5,\n",
    "    show_all_steps=True,\n",
    "    policy=\"trained\",\n",
    "    render_interval=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A and B in the same column (column 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_col_env = MultiAgentEnvironment(\n",
    "    grid_size=5,\n",
    "    num_agents=4,\n",
    "    package_position=(0,2),\n",
    "    target_position=(4,2),\n",
    "    rewards=REWARDS\n",
    ")\n",
    "same_col_results = test_trained_agents(\n",
    "    same_col_env,\n",
    "    q_learning,\n",
    "    num_tests=1,\n",
    "    steps_per_test=200,\n",
    "    render=True,\n",
    "    render_delay=0.5,\n",
    "    show_all_steps=True,\n",
    "    policy=\"trained\",\n",
    "    render_interval=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the previous section, to increase our % of success it is necessary to add another training stage with the scenarios of A and B aligned so that our agents learn to take different paths in these situations depending on the direction (if they have the package or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Anthropic. (2024). Claude 3.5/3.7 Sonnet [Large language model]. https://www.anthropic.com/\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Significant portions of the code were developed in collaboration with Anthropicâ€™s Claude 3.7 Sonnet large language model, following my instructions and design ideas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
